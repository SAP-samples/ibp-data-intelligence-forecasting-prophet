{
	"component": "com.sap.system.python3Operator",
	"description": "Fetch Forecast Input",
	"inports": [
		{
			"name": "requestId",
			"type": "scalar",
			"vtype-ID": "com.sap.core.int64"
		}
	],
	"outports": [
		{
			"name": "data",
			"type": "string"
		},
		{
			"name": "error",
			"type": "string"
		},
		{
			"name": "info",
			"type": "string"
		}
	],
	"tags": {},
	"subenginestags": {},
	"config": {
		"$type": "http://sap.com/vflow/FetchForecastInput.configSchema.json",
		"script": "import requests\nimport datetime\nimport json\n\n\n# get information from Configuration and Connection\nconn = api.config.ODataConnection[\"connectionProperties\"]\nbase_url = conn[\"url\"]\nuser = conn[\"user\"]\npassword = conn[\"password\"]\n\n\ndef parse_param_string(algorithm_parameter_string):\n    \"\"\" Parse algorithm parameters from str to dict. \"\"\"\n\n    algorithm_params = {}\n    for alg_parameter in algorithm_parameter_string.split(\";\"):\n        if \"=\" in alg_parameter:\n            key, value = alg_parameter.split(\"=\")\n            algorithm_params[key] = value\n\n    return algorithm_params\n\n\ndef parse_timestamps(time_periods):\n    \"\"\" Parse time information into a compact format of lists. \"\"\"\n    start_time = [None] * len(time_periods)\n    end_time = [None] * len(time_periods)\n    description = [None] * len(time_periods)\n\n    for element in time_periods:\n        idx = element[\"PeriodIndex\"] - 1    # list indices start at 0\n        start_time[idx] = element[\"StartTimeStamp\"]\n        end_time[idx] = element[\"EndTimeStamp\"]\n        description[idx] = element[\"Description\"]\n\n    return {\"StartTimeStamp\": start_time, \"EndTimeStamp\": end_time, \"Description\": description}\n\n\ndef parse_input_data(planning_objects, historical_periods, forecast_periods):\n    \"\"\" Format input data into a format which makes processing easier. (e.g. converting the time series to actual numbers) \"\"\"\n    data_dicts = []\n    for planning_object in planning_objects:\n        # format master data\n        master_data = {entry[\"AttributeID\"]: entry[\"AttributeValue\"] for entry in planning_object[\"_MasterData\"]}\n\n        # format key figure data\n        algorithm_data = {}\n        for input_data in planning_object[\"_AlgorithmDataInput\"]:\n            time_series_string = input_data[\"TimeSeries\"]\n            time_series = [float(x) for x in time_series_string.split(';')]\n            front_padding = input_data[\"FirstPeriodIndex\"] - 1\n            end_padding = forecast_periods if (len(time_series) + front_padding) == historical_periods else 0\n            algorithm_data[input_data[\"SemanticKeyFigure\"]] = [None] * front_padding + time_series + [None] * end_padding\n\n        data_dicts.append({\"group_id\": planning_object[\"GroupID\"],\n                           \"master_data\": master_data,\n                           \"algorithm_data\": algorithm_data\n                           })\n    return data_dicts\n\n\n\ndef fetch_algorithm_data(request_id):\n    \"\"\" Fetch forecast model details and input data (= master data and planning objects). \"\"\"\n    with requests.Session() as session:\n        # Fetch Forecast model details\n        param_url = f\"{base_url}/Request?$filter=RequestID%20eq%20{request_id}&$expand=_TimePeriod\" \n        param_request = session.get(param_url, headers={\"accept\": \"application/json\"}, auth=(user, password))\n        assert param_request.status_code == 200, f\"Forecast model details could not be fetched. Status code: {param_request.status_code}\"\n        params = param_request.json()[\"value\"][0]   # list with one element since filtered by request id\n        api.logger.info(f\"[DI LOG] Parameters fetched: {params}\")\n        api.send(\"info\", f\"[Request {request_id}] Parameters were fetched: {params}\")\n\n        # Fetch Algotihm Data Input (master data + key figures)\n        input_url = f\"{base_url}/Input?$filter=RequestID%20eq%20{request_id}&$expand=_AlgorithmDataInput,_MasterData\"\n        planning_objects = get_data([], input_url)\n        api.logger.info(f\"[DI LOG] Algorithm data fetched: {planning_objects}\")\n        api.send(\"info\", f\"[Request {request_id}] Algorithm data fetched: {planning_objects}\")\n\n    return params, planning_objects\n\n\ndef get_data(planning_objects, url: str, cookies=None):\n    \"\"\" Fetch algorithm input data recursively. \"\"\"\n    if cookies:\n        data_get = requests.get(f\"{url}\", headers={\"accept\": \"application/json\"}, cookies=cookies, verify=False)\n    else:\n        data_get = requests.get(f\"{url}\", headers={\"accept\": \"application/json\"}, auth=(user, password))\n        cookies = data_get.cookies\n    body = data_get.json()\n    assert data_get.status_code == 200, f\"Planning objects could not be fetched. Status code: {data_get.status_code}\"\n    body = data_get.json()\n    \n\n    if body[\"value\"]:\n        planning_objects.extend(body[\"value\"])\n    else:\n        return planning_objects\n\n    # recursively get remaining data\n    if \"@odata.nextLink\" in body.keys():\n        odata_link = body['@odata.nextLink']\n        return get_data(planning_objects, f\"{base_url}/Input{odata_link[odata_link.index('?'):]}\", cookies)\n    else:\n        return planning_objects\n\n\ndef get_and_prepare_data(request_id):\n    \"\"\" Fetch data and preprocess it while handling potential errors. \"\"\"\n    try:\n        start = datetime.datetime.now()\n        api.logger.info(f\"[DI LOG] Data is requested for request id {request_id}.\")\n\n        # fetch data via OData service\n        params, planning_objects = fetch_algorithm_data(request_id)\n        api.logger.info(f\"[DI LOG] Data is received for request id {request_id}.\")\n\n        # preprocess parameters\n        params[\"AlgorithmParameter\"] = parse_param_string(params[\"AlgorithmParameter\"])\n        params[\"TimePeriod\"] = parse_timestamps(params[\"_TimePeriod\"])\n        del params[\"_TimePeriod\"]\n\n        # preprocess input data\n        data_dicts = parse_input_data(planning_objects, params[\"HistoricalPeriods\"], params[\"ForecastPeriods\"])\n        delta = datetime.datetime.now() - start\n        api.logger.info(f\"[DI LOG] Finished pre-processing data for request id {request_id}.\"\n                        f\" Fetching and preprocessing data took {delta.total_seconds()} seconds.\")\n        api.send(\"data\", json.dumps({\"parameters\": params, \"data_dicts\": data_dicts}))\n        api.send(\"info\", f\"[Request {request_id}] Data was preprocessed.\")\n    except AssertionError as assertion_err:\n        api.send(\"error\", json.dumps({\"error\": f\"{assertion_err}\",\n                                      \"request_id\": request_id}))\n        api.logger.error(f\"[DI LOG] {assertion_err}\")\n    except Exception as e:\n        api.send(\"error\", json.dumps({\"error\": \"Input data could not be converted to the necessary format.\",\n                                      \"request_id\": request_id}))\n        api.logger.error(f\"[DI LOG] {e}\")\n\n\n\napi.set_port_callback(\"requestId\", get_and_prepare_data)\n\n"
	},
	"versionStatus": "active",
	"icon": "puzzle-piece",
	
	"_comment": {
        	"SPDX-FileCopyrightText": "2022 SAP", 
        	"SESPDX-License-Identifier": "Apache-2.0"
                }
}
